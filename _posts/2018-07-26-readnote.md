---
layout: post
published: Ture
category: "Image Registration"
title: Max Jaderberg 2016 (26 Jul 2018 Reading Notes)
subtitle: Spatial Transformer Networks
image: https://static.wixstatic.com/media/eb8166_65b1e81195b04c21adbbe6521cb8afc5~mv2.png/v1/fill/w_321,h_241,al_c,lg_1/eb8166_65b1e81195b04c21adbbe6521cb8afc5~mv2.png
tags: [Reading Notes, Image Registration]
---

**Title:** Spatial Transformer Networks [Paper Link](https://arxiv.org/abs/1506.02025)

**Authors:** Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu

**Association:** Google DeepMind, London, UK

**Submission:** Feb 2016

![](https://davidstutz.de/wordpress/wp-content/uploads/2018/02/jaderberg_1.jpg) 


.................................................................................................................................

## >>> Background of Deep Learning in Medical Image Analysis

[[NEED A Reading Notes]](https://xuuuuuuchen.github.io/2018-08-01-DeepLearninginMedicalImageAnalysis/)

## >>> A Review: Robot-Assisted Endovascular Catheterization Technologies: 

[[NEED A Reading Notes]](https://xuuuuuuchen.github.io/Robot-AssistedEndovascularCatheterizationTechnologies/)

.................................................................................................................................
## >>> Image Registration Basic Knowledge

[[Basic Knowledge]](https://xuuuuuuchen.github.io/2018-07-31-ImageRegistration-basic/)

## >>> Image Registration Literature Review

[[Literature Review]](https://xuuuuuuchen.github.io/2018-07-31-ImageRegistration/)

## >>> Slice-To-Volume Medical Image Registration Background

[[NEED A Reading Notes]](https://xuuuuuuchen.github.io/2018-08-01-ImageRegistration-2D-3D/)

.................................................................................................................................

## Contributions

![](https://image.slidesharecdn.com/06spatialtransformernetworks-160329154407/95/spatial-transformer-networks-25-1024.jpg?cb=1459266300) 


## Spatial Transformer Networks (STNs)

The Spatial Transformer mechanism addresses the issues above by providing Convolutional Neural Networks with explicit spatial transformation capabilities. It possesses 3 defining properties that make it very appealing.

**modular:** STNs can be **inserted** anywhere into existing architectures with relatively small tweaking (对…稍作调整).

**differentiable:** STNs can be **trained** with backprop allowing for end-to-end training of the models they are injected in.

**dynamic:** STNs perform active spatial transformation on a feature map for each input sample as compared to the pooling layer which acted identically for all input samples.

![](https://davidstutz.de/wordpress/wp-content/uploads/2018/02/jaderberg_1.jpg)
 
The Spatial Transformer module consists in three components: 

**1. localisation network**

[goal] spit out the parameters **θ** of the affine transformation that’ll be applied to the input feature map.

[input] feature map U of shape (H, W, C)

[output]  transformation matrix **θ** of shape (6,)

[architecture] fully-connected network or ConvNet

**2. grid generator**

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-26-readnote/2.png?raw=true) 

[goal] to output a **parametrised sampling grid**, which is a set of points where the input map should be sampled to produce the desired transformed output.

[architecture]

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-26-readnote/3.png?raw=true) 

**3. sampler**

[goal] To perform a spatial transformation of the input feature map, a sampler must take the set of sampling points Tθ(G), along with the input feature map U and produce the sampled output feature map V.

[architecture]
![](https://image.slidesharecdn.com/06spatialtransformernetworks-160329154407/95/spatial-transformer-networks-15-638.jpg?cb=1459266300) 


It is also possible to use spatial transformers to **downsample or oversample** a feature map, as one can define the output dimensions H' and W' to be different to the input dimensions H and W. However, with sampling kernels with a fixed, small spatial support (such as the bilinear kernel), downsampling with a spatial transformer can cause aliasing effects.

## Performance
![](https://kevinzakka.github.io/assets/stn2/epoch_evolution.gif) ![](https://kevinzakka.github.io/assets/stn2/moving_evolution.gif) 


## Reference

Spatial transformer networks SLIDESAHRE
[link](https://www.slideshare.net/xavigiro/spatial-transformer-networks) 

Deep Learning Paper Implementations: Spatial Transformer Networks:

[link1](https://kevinzakka.github.io/2017/01/10/stn-part1/) 

[link2](https://kevinzakka.github.io/2017/01/18/stn-part2/) 


