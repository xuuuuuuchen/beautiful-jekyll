---
layout: post
published: Ture
category: "Image Registration"
title: 26 Jul 2018 Reading Notes
subtitle: ssEMnet (Serial-Section Electron Microscopy Image Registration Using a Spatial Transformer Network with Learned Features)
image: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRAUYBdlX5AoNbjJ9TbuGN9Wry_ceoFoqB4a7hPEUGI86a6XYdeog

tags: [Reading Note, Convolutional Neural Network, Image Registration]
---

**Title:** ssEMnet: Serial-Section Electron Microscopy Image Registration Using a Spatial Transformer Network with Learned Features [Paper Link](https://arxiv.org/abs/1707.07833)

**Authors:** Inwan Yoo, David G. C. Hildebrand, Willie F. Tobin, Wei-Chung Allen Lee, Won-Ki Jeong

**Association:** Ulsan National Institute of Science and Technology, Ulsan, South Korea

**Submission:** Dec 2017

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRAUYBdlX5AoNbjJ9TbuGN9Wry_ceoFoqB4a7hPEUGI86a6XYdeog) 



## Contributions

![](https://image.slidesharecdn.com/06spatialtransformernetworks-160329154407/95/spatial-transformer-networks-25-1024.jpg?cb=1459266300) 

## Intro...

introducing two very important concepts that will prove crucial in understanding the inner workings of the Spatial Transformer layer. 

**affine transformations**

https://kevinzakka.github.io/2017/01/10/stn-part1/
![](https://kevinzakka.github.io/assets/stn/affine.png) 

When an image undergoes an affine transformation such as a rotation or scaling, the pixels in the image get moved around. This can be especially problematic when a pixel location in the output does not map directly to one in the input image.

*To solve this problem:*

In the illustration below, you can clearly see that the rotation places some points at locations that are not centered in the squares. This means that they would not have a corresponding pixel value in the original image.

![](https://kevinzakka.github.io/assets/stn/stickman.png) 

So for example, suppose that after rotating an image, we need to find the pixel value at the location ***(6.7, 3.2)***. The problem with this is that there is no such thing as fractional pixel locations.

**bilinear interpolation**

![](https://kevinzakka.github.io/assets/stn/interpol.png) 


![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-26-readnote/1.png?raw=true) 



## Background

Wu et al. [[Paper]](https://ieeexplore.ieee.org/document/7314894/): "used a convolutional stacked auto-encoder (**CAE**) to extract features from fixed and moving images that are subsequently used in conventional deformable image registration algorithms. However, the CAE is decoupled from the image registration task and hence, it does not necessarily extract the features most descriptive for image registration. The training of the CAE was unsupervised, but the registration task was not learned end-to-end."

Miao et al. [[Reading Note]](https://xuuuuuuchen.github.io/2018-07-23-readnote-add/) "used a convolutional neural network (ConvNet) regressor to predict a transformation matrix for rigid registration of synthetic 2D to 3D images. "

Liao et al. [[Paper]](https://arxiv.org/abs/1611.10336) "used a ConvNet for intra-patient rigid registration of CT to cone-beam CT applied to either cardiac or abdominal images. "

"Both registration methods (Miao et al. and Liao et al.) were supervised: for training, transformation parameters were generated, which is task specific and highly challenging."

Jaderberg et al. [[Reading Note]](https://xuuuuuuchen.github.io/2018-07-26-readnote/) [[Paper]](https://arxiv.org/abs/1506.02025)  *(‎Cited by 901)*: " introduced the spatial transformer network (STN) that can be used as a **building block that aligns input images in a larger network** that performs a particular task. By training the entire network end-to-end, the embedded STN deduces optimal alignment for solving that specific task. "
"**However, alignment is not guaranteed**, and it is only performed when required for the task of the entire network. The STNs were used for affine transformations, as well as deformable transformations using thin-plate splines. However, an STN needs many labeled training examples, and to the best of our knowledge,** **have not yet been used in medical imaging**."

## Spatial Transformer Networks (STNs)

The Spatial Transformer mechanism addresses the issues above by providing Convolutional Neural Networks with explicit spatial transformation capabilities. It possesses 3 defining properties that make it very appealing.

**modular:** STNs can be **inserted** anywhere into existing architectures with relatively small tweaking (对…稍作调整).

**differentiable:** STNs can be **trained** with backprop allowing for end-to-end training of the models they are injected in.

**dynamic:** STNs perform active spatial transformation on a feature map for each input sample as compared to the pooling layer which acted identically for all input samples.

![](https://davidstutz.de/wordpress/wp-content/uploads/2018/02/jaderberg_1.jpg)
 
The Spatial Transformer module consists in three components: 

**1. localisation network**

[goal] spit out the parameters **θ** of the affine transformation that’ll be applied to the input feature map.

[input] feature map U of shape (H, W, C)

[output]  transformation matrix **θ** of shape (6,)

[architecture] fully-connected network or ConvNet

**2. grid generator**

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-26-readnote/2.png?raw=true) 

[goal] to output a **parametrised sampling grid**, which is a set of points where the input map should be sampled to produce the desired transformed output.

[architecture]

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-26-readnote/3.png?raw=true) 

**3. sampler**

[goal] To perform a spatial transformation of the input feature map, a sampler must take the set of sampling points Tθ(G), along with the input feature map U and produce the sampled output feature map V.

[architecture]
![](https://image.slidesharecdn.com/06spatialtransformernetworks-160329154407/95/spatial-transformer-networks-15-638.jpg?cb=1459266300) 


It is also possible to use spatial transformers to **downsample or oversample** a feature map, as one can define the output dimensions H' and W' to be different to the input dimensions H and W. However, with sampling kernels with a fixed, small spatial support (such as the bilinear kernel), downsampling with a spatial transformer can cause aliasing effects.

## Performance
![](https://kevinzakka.github.io/assets/stn2/epoch_evolution.gif) ![](https://kevinzakka.github.io/assets/stn2/moving_evolution.gif) 


## Ref.

Spatial transformer networks SLIDESAHRE
[link](https://www.slideshare.net/xavigiro/spatial-transformer-networks) 

Deep Learning Paper Implementations: Spatial Transformer Networks:

[link1](https://kevinzakka.github.io/2017/01/10/stn-part1/) 

[link2](https://kevinzakka.github.io/2017/01/18/stn-part2/) 


