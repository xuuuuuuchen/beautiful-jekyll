---
layout: post
published: Ture
title: 20 Jul 2018 Reading Note
subtitle: RetinaNet (Focal Loss for Dense Object Detection)
image: /img/2018-07-19-readnote/logo.png
tags: [Reading Note, Convolutional Neural Network, Object Detection]
---

**Title:** Focal Loss for Dense Object Detection [Paper Link](https://arxiv.org/abs/1708.02002)

**Authors:** Tsung-Yi Lin, Priya Goyal, Ross Girshick, **Kaiming He**, Piotr Dollar

**Association:** Facebook AI Research (FAIR)

**Submission:** Feb 2018

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-20-readnote/1.png?raw=true) 

## Contuibutions

**(1)**  RetinaNet is essentially a Feature Pyramid Network (FPN) [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/) with the cross-entropy loss replaced by Focal loss. 

**(2)**  **DUE TO** class imbalance during training (the main obstacle) impeding (阻碍; 妨碍) one-stage detector, a new loss function called ==**Focal loss**== which significantly increased the accuracy.

{: .box-note}
**In other words,** the focal loss performs the opposite role of a robust loss: it focuses training on a sparse set of hard examples.

## Background

Current state-of-the-art object detectors are based on a ==two-stage, proposal-driven mechanism.==

>Q: What is two-stage object detectors? 
>The **first stage** generates a sparse set of candidate object locations. 
The **second stage** classifies each candidate location as one of the foreground classes or as background using a convolutional neural network.


>Q: What is one-stage object detectors? 
> 

{: .box-warning}
**However:** could a simple one-stage detector achieve similar accuracy?



CNN based object detectors can be categorized into:

| one-stage detector | two-stage detectors |
| :------ |:--- | 
| YOLO [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/) | Faster R-CNN [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/)| 
| SSD [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/) | R-CNN [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/)| 
| RetinaNet [Reading Note](https://xuuuuuuchen.github.io/2018-07-20-readnote/) | FPN [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/)| 
| OverFeat [Reading Note](https://xuuuuuuchen.github.io/2018-07-20-readnote/) | | 


## Motivation

**Class Imbalance Problem:**
1. Training is inefficient as most locations are easy negatives that ==contribute no useful== learning signal; 
2. the easy negatives can overwhelm training and lead to degenerate models. 

**Common Solution:** 
To perform some form of **hard negative mining** that samples hard examples during training 
**OR** more complex sampling/reweighing schemes 
**OR**  introduce a weighting factor α treated as a hyperparameter to set ==to balances the importance of positive/negative examples but it does not differentiate between easy/hard examples.==

**Their Solution:** 
focal loss **naturally** handles the class imbalance and **efficiently** trains on all examples **without** sampling and  **without** easy negatives overwhelming the loss and computed gradients ==by reshaping the loss function to down-weight easy examples and thus focus training on hard negatives.==

## Focal Loss

To address the problem: imbalance between foreground and background classes during training.

**focal loss:**
More formally, we propose to add a modulating factor:
(1 − pt)^γ
to the cross entropy loss, with tunable focusing parameter γ ≥ 0. 
We define the focal loss as:
![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-20-readnote/loss.png?raw=true) 

In practice we use an α-balanced variant of the focal loss

FL(pt) = −αt(1−pt)^γ log(pt).


## Performance

There are two key-points in object detection evaluation, the one is average precision (**AP**) and the other is average recall (**AR**). 

AR means how much objects we can find out, 
AP means how much objects is correctly predicted (right label for classification). 

AP and AR are usually evaluated on different IoU threshold to **validate the regression capability** for object location. The larger IoU is, the more accurate regression needs. 

AP and AR are also evaluated on **different range of bounding box areas** (small, middle, and large) to find the detail influences on the scale objects.

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-19-readnote/result3.png?raw=true) 

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-19-readnote/result4.png?raw=true) 

## Other Useful Info.

Training strategies provided by **facebook Research Detectron repository**: [Github Link]( https://github.com/facebookresearch/Detectron)

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-19-readnote/seg.png?raw=true) 

