---
layout: post
published: Ture
title: 20 Jul 2018 Reading Note
subtitle: RetinaNet (Focal Loss for Dense Object Detection)
image: /img/2018-07-19-readnote/logo.png
tags: [Reading Note, Convolutional Neural Network, Object Detection]
---

**Title:** Focal Loss for Dense Object Detection [Paper Link](https://arxiv.org/abs/1708.02002)

**Authors:** Tsung-Yi Lin, Priya Goyal, Ross Girshick, **Kaiming He**, Piotr Dollar

**Association:** Facebook AI Research (FAIR)

**Submission:** Feb 2018

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-20-readnote/1.png?raw=true) 

## Contuibutions

**(1)**  RetinaNet is essentially a Feature Pyramid Network (FPN) [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/) with the cross-entropy loss replaced by Focal loss. 

**(2)**  **DUE TO** class imbalance during training (the main obstacle) impeding (阻碍; 妨碍) one-stage detector, a new loss function called ==**Focal loss**== which significantly increased the accuracy.

{: .box-note}
**In other words,** the focal loss performs the opposite role of a robust loss: it focuses training on a sparse set of hard examples.

## Background

Current state-of-the-art object detectors are based on a ==two-stage, proposal-driven mechanism.==

>Q: What is two-stage object detectors? 
>The **first stage** generates a sparse set of candidate object locations. 
The **second stage** classifies each candidate location as one of the foreground classes or as background using a convolutional neural network.


>Q: What is one-stage object detectors? 
> 

{: .box-warning}
**However:** could a simple one-stage detector achieve similar accuracy?



CNN based object detectors can be categorized into:

| one-stage detector | two-stage detectors |
| :------ |:--- | 
| YOLO [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/) | Faster R-CNN [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/)| 
| SSD [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/) | R-CNN [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/)| 
| RetinaNet [Reading Note](https://xuuuuuuchen.github.io/2018-07-20-readnote/) | FPN [Reading Note](https://xuuuuuuchen.github.io/2018-07-19-readnote/)| 
| OverFeat [Reading Note](https://xuuuuuuchen.github.io/2018-07-20-readnote/) | | 


## Motivation 1
**Class Imbalance Problem:**

1. Training is inefficient as most locations are easy negatives that ==contribute no useful== learning signal; 
2. the easy negatives can overwhelm training and lead to degenerate models. 

**Common Solution:** is to perform some form of **hard negative mining** that samples hard examples during training **OR** more complex sampling/reweighing schemes.

**Their Solution:** focal loss **naturally** handles the class imbalance and **efficiently** trains on all examples **without** sampling and  **without** easy negatives overwhelming the loss and computed gradients.

## Network Structure

To address these top problems, DetNet has following characteristics:

(i) The number of stages is **directly designed** for Object Detection. **------>** DetNet has exactly the same number of stages as the detector used, therefore extra stages like P6 can be pre-trained in ImageNet dataset.)

(ii) Even though involving more stages (such as 6 stages or 7 stages) than traditional classification network, we maintain high spatial resolution of the feature maps, while keeping large receptive field  **------>** Det- Net is more powerful in **locating** the boundary of large objects and **finding** the missing small objects.

ResNet-50 as our baseline.

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-19-readnote/denet.png?raw=true) 

## Performance

There are two key-points in object detection evaluation, the one is average precision (**AP**) and the other is average recall (**AR**). 

AR means how much objects we can find out, 
AP means how much objects is correctly predicted (right label for classification). 

AP and AR are usually evaluated on different IoU threshold to **validate the regression capability** for object location. The larger IoU is, the more accurate regression needs. 

AP and AR are also evaluated on **different range of bounding box areas** (small, middle, and large) to find the detail influences on the scale objects.

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-19-readnote/result3.png?raw=true) 

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-19-readnote/result4.png?raw=true) 

## Other Useful Info.

Training strategies provided by **facebook Research Detectron repository**: [Github Link]( https://github.com/facebookresearch/Detectron)

![](https://github.com/xuuuuuuchen/xuuuuuuchen.github.io/blob/master/img/2018-07-19-readnote/seg.png?raw=true) 

